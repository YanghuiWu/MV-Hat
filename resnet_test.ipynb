{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import ssl\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_dataset import HatsDataset\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channel = 3\n",
    "num_class = 15\n",
    "learning_rate = 1e-3\n",
    "batch_size = 20\n",
    "num_epochs = 1\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.RandomCrop(256,padding=3,padding_mode='constant'),\n",
    "     transforms.RandomHorizontalFlip(),\n",
    "      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "dataset = HatsDataset(csv_file='HatsOnly.csv',root_dir='HatsOnly',\n",
    "    transform=transform) #8780\n",
    "\n",
    "humanset = HatsDataset(csv_file='HumanOnly.csv',root_dir='HumanOnly',\n",
    "    transform=transform) \n",
    "\n",
    "trainset, testset = torch.utils.data.random_split(dataset,[6000,1138])\n",
    "\n",
    "trainset_human, testset_human = torch.utils.data.random_split(humanset,[1312,327])\n",
    "\n",
    "humanloader =DataLoader(testset_human, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "trainloader =DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testloader = DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=2)\n",
    "\n",
    "trainset_mix = torch.utils.data.ConcatDataset([trainset,trainset_human])\n",
    "testset_mix = torch.utils.data.ConcatDataset([testset,testset_human])\n",
    "\n",
    "trainloader_mix = DataLoader(trainset_mix, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=2)\n",
    "\n",
    "testloader_mix = DataLoader(testset_mix, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=2)\n",
    "\n",
    "classes = ('baseballcap', 'BikeHelmet', 'BucketHat', 'CowboyHat',\n",
    "           'FeltHat', 'FireFighterHat', 'FlatCap', 'GraduationCap', 'Heaterhat', 'MilitaryHelmet',\n",
    "           'MotorCycle Helmet','Police Hat','SateftyHelmet','TopHat','beanie')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)\n",
    "\n",
    "\n",
    "trainloader = DeviceDataLoader(trainloader,'cuda')\n",
    "testloader = DeviceDataLoader(testloader,'cuda')\n",
    "humanloader = DeviceDataLoader(humanloader,'cuda')\n",
    "\n",
    "\n",
    "trainloader_mix = DeviceDataLoader(trainloader_mix,'cuda')\n",
    "testloader_mix = DeviceDataLoader(testloader_mix,'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2dModuel(in_channel,out,K_sz=3,pad=1,st=1):\n",
    "    hidden_layer=[nn.Conv2d(in_channel,out,kernel_size=K_sz,padding=pad,stride=st),\n",
    "                nn.BatchNorm2d(out),\n",
    "                nn.LeakyReLU(inplace=True)]\n",
    "    return nn.Sequential(*hidden_layer)\n",
    "\n",
    "\n",
    "\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,connect=True):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "        self.connect = connect\n",
    "        self.conv1 = nn.Sequential(conv2dModuel(in_channels,out_channels),\n",
    "                                   nn.MaxPool2d(kernel_size=2, stride=2), #downsize\n",
    "                                   conv2dModuel(out_channels,out_channels))\n",
    "        self.conv2 = nn.Sequential(conv2dModuel(out_channels,out_channels),\n",
    "                                   conv2dModuel(out_channels,out_channels))\n",
    "        self.relu = nn.LeakyReLU(inplace=True)\n",
    "    def forward(self,x):\n",
    "        if self.connect==True:\n",
    "            y = self.conv1(x)\n",
    "        else:\n",
    "            y = self.conv2(x)\n",
    "            y = y+x    \n",
    "        z = self.conv2(y)\n",
    "        z = z+y\n",
    "        z = self.relu(z)\n",
    "        return z\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\n",
    "        self.conv1 = conv2dModuel(3,64,K_sz=7,pad=3,st=2)\n",
    "        self.conv2 = ResNetBlock(64,64,connect=False)\n",
    "        self.conv3 = ResNetBlock(64,128)\n",
    "        self.conv4 = ResNetBlock(128,256)\n",
    "        self.conv5 = ResNetBlock(256,512)\n",
    "        self.classifier = nn.Sequential(nn.MaxPool2d(4),\n",
    "                                       nn.Flatten(),\n",
    "                                       nn.Linear(512*4*4,256),\n",
    "                                       nn.LeakyReLU(),\n",
    "                                       nn.Linear(256,128),\n",
    "                                       nn.LeakyReLU(),\n",
    "                                       nn.Linear(128,15)\n",
    "                                       )\n",
    "        \n",
    "    def forward(self, x):\n",
    "  \n",
    "        x = self.conv1(x) #layer 1\n",
    "        x = self.conv2(x) #layer 2,3,4,5\n",
    "        x = self.conv3(x) #layer 6,7,8,9\n",
    "        x = self.conv4(x) #layer 10,11,12,13\n",
    "        x = self.conv5(x) #layer 14,15,16,17\n",
    "        x = self.classifier(x) #(18)\n",
    "        return x\n",
    "        #y = self.fc4(x)\n",
    "        # YOUR CODE HERE\n",
    "        #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea921009a26f4c05a79952c2d78fdf36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "net = ResNet18().cuda()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.002)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "# comment this line of code if not on gpu\n",
    "from tqdm.notebook import tqdm\n",
    "losses = []\n",
    "for epoch in tqdm(range(50)):  # loop over the dataset multiple times\n",
    "    loss_temp=0.0      \n",
    "    for inputs,label in trainloader_mix:\n",
    "        with torch.cuda.amp.autocast():\n",
    "             outputs = net(inputs)\n",
    "             loss = criterion(outputs,label.cuda())\n",
    "        #outputs = net(inputs)\n",
    "        #loss = criterion(outputs,label)\n",
    "        scaler.scale(loss).backward()\n",
    "        #loss.backward()\n",
    "        #optimizer.step() \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "        loss_temp += loss.item()    \n",
    "    losses = np.append(losses,loss_temp)\n",
    "    # YOUR CODE HERE\n",
    "plt.plot(losses)\n",
    "    #raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "851433d38881c439cd1aa4ab1f9a2958ccad8ff830098145c1b18d13f5b5af71"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
